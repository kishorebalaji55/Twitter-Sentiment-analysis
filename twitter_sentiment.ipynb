{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEWgK7KUKOh3"
   },
   "source": [
    "# Sentiment Detection in Tweets\n",
    "we will be classifying tweets into \n",
    "\n",
    "* positive\n",
    "* Negative\n",
    "\n",
    "link to download the dataset:\n",
    "(https://drive.google.com/u/0/uc?export=download&confirm=vN3O&id=1xnIo9sCIQ4ETvG8v3_-1wPhMl9mHCa5i) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TA5XOHTsKIq"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btA4CBReMeGB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"twitter_sentiments.csv\",encoding = \"ISO-8859-1\",header=None, names = [ 'sentiment', 'id', 'date', 'flag', 'user', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "f53juNBNM4Q7",
    "outputId": "fa108ae8-21b8-4b5f-8fb5-88706a46e418"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  ...                                               text\n",
       "0          0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  ...  is upset that he can't update his Facebook by ...\n",
       "2          0  ...  @Kenichan I dived many times for the ball. Man...\n",
       "3          0  ...    my whole body feels itchy and like its on fire \n",
       "4          0  ...  @nationwideclass no, it's not behaving at all....\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "7qC2ntzTM8OW",
    "outputId": "be9104d3-974a-45c9-edcd-ba908b2dad32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      "sentiment    1600000 non-null int64\n",
      "id           1600000 non-null int64\n",
      "date         1600000 non-null object\n",
      "flag         1600000 non-null object\n",
      "user         1600000 non-null object\n",
      "text         1600000 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "BIRFFAnKNYcL",
    "outputId": "ccd02d83-8fe8-4670-a950-f61a5aeb4f0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeyH6aXUtNEg"
   },
   "source": [
    "**{4**: **Positive** ,\n",
    "**0**: **Negative}**\n",
    "\n",
    "There are 16,00,000 tweets\n",
    "\n",
    "For our initial training purposes, let us take only 10K tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MC0Td2yktfeJ"
   },
   "outputs": [],
   "source": [
    "pos = data[data[\"sentiment\"] == 4][:5000]\n",
    "neg = data[data[\"sentiment\"] == 0][:5000]\n",
    "\n",
    "data = pd.concat([pos,neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "jOHuZIh4t-zm",
    "outputId": "47ba3735-da45-4e7d-f93f-6d64e2ba7b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 800000 to 4999\n",
      "Data columns (total 6 columns):\n",
      "sentiment    10000 non-null int64\n",
      "id           10000 non-null int64\n",
      "date         10000 non-null object\n",
      "flag         10000 non-null object\n",
      "user         10000 non-null object\n",
      "text         10000 non-null object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 546.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RzxosGBuCgC"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t8ERKDOOOIat"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "  # Remove all the special characters\n",
    "  processed_feature = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "  # remove all single characters\n",
    "  processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "  # Remove single characters from the start\n",
    "  processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "  # Substituting multiple spaces with single space\n",
    "  processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "  # Converting to Lowercase\n",
    "  processed_feature = processed_feature.lower()\n",
    "\n",
    "  return processed_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdlQLgmOPHCX"
   },
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "W22nv7zuQ4vK",
    "outputId": "b39b8c88-b8b0-47f1-97f8-fbe6ab300550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_yCsXPjuPjX"
   },
   "source": [
    "## Training\n",
    "\n",
    "The computer cannot understand text, we need to represent text in numbers. Let us go step by step on how to perform that.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 1. Creating Vocabulary\n",
    "The set of unique words used in the text corpus is referred to as the vocabulary. When processing raw text for NLP, everything is done around the vocabulary.\n",
    "\n",
    "\n",
    "```\n",
    "texts = ['bob ate apples', 'fred ate apples', 'bob ate pears']\n",
    "convert_to_vocab(texts)\n",
    "\n",
    ">> Output: ['bob', 'fred', 'ate', 'apples', 'pears']\n",
    "```\n",
    "We can create all the sentences in **texts** using the vocab list.\n",
    "\n",
    "### 2. Tokenization\n",
    "We can use the vocabulary to find the number of times each word appears in the corpus, figure out which words are the most common or uncommon, and filter each text document based on the words that appear in it. However, the most important part of the vocabulary is that it allows us to represent each piece of text by the specific words that appear in it.\n",
    "\n",
    "Rather than being represented as one long string, a piece of text can be represented as a vector/list of its vocabulary words. This process is known as tokenization, where each individual vocabulary word in a piece of text is a token.\n",
    "\n",
    "\n",
    "```\n",
    "texts = ['bob ate apples, pears', 'fred ate apples!']\n",
    "tokenize(texts)\n",
    ">> Output: [['bob', 'ate', 'apples', 'pears'], ['fred', 'ate', 'apples']]\n",
    "```\n",
    "**Note that the punctuations are gone**\n",
    "\n",
    "### 3. Embeddings\n",
    "We need to represent the above as numbers for our machine to understand. A simple way to do it would be using the list index from the vocabulary.\n",
    "```\n",
    "[['bob', 'ate', 'apples', 'pears'], ['fred', 'ate', 'apples']]\n",
    "can be represented as\n",
    "[[0, 3, 4,5], [1, 3, 4]]\n",
    "using this key value pair\n",
    "{'ate': 3, 'apples': 4, 'bob': 0, 'pears': 5, 'fred': 1}\n",
    "```\n",
    "\n",
    "We have now represented vocabulary words with unique integer IDs. However, these integer IDs don't give a sense of how different words may be related.\n",
    "\n",
    "The solution to this problem is to convert each word into an embedding vector. An embedding vector is a higher-dimensional vector representation of a vocabulary word. Since vectors have a sense of distance (as they are just points in a higher-dimensional space), embedding vectors give us a word representation that captures relationships between words.\n",
    "\n",
    "Head over to https://projector.tensorflow.org/ to visualize word vectors.\n",
    "\n",
    "We will be using Tfidf to convert our words unto vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAhPKSk4PcLC"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer (stop_words=stopwords.words('english'))\n",
    "tfidf = vectorizer.fit_transform(data[\"text\"]).toarray()\n",
    "# fit is used to create the vocab and transform returns the word representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VwrQfp8P2VD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, data[\"sentiment\"], test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7agqavvV8Hf9"
   },
   "source": [
    "We will be using a **random Forest** Classifier to train our model. More information on random forests [here](https://www.youtube.com/watch?v=eM4uJ6XGnSM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "bMyeiWsPP52g",
    "outputId": "88e30d94-232d-41b0-8e34-5b36057104e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr-ZyUJQuVAO"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zB5AiOOpQEfw"
   },
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "eSrtw7wrQFu8",
    "outputId": "629956d6-e69d-4dbe-f72f-233c7e3089c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZO12l5wkUQFc"
   },
   "outputs": [],
   "source": [
    "sample_tweets = [\"I did not like the last weeks episode\",\"Their customer support is doing a good job\"]\n",
    "tfidf2 = vectorizer.transform(sample_tweets).toarray()\n",
    "predictions = text_classifier.predict(tfidf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "f54EOvKnUolt",
    "outputId": "0ca16a78-e9da-4003-950a-372e58871743"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "twitter_sentiment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
